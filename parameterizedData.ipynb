{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readParameters import readParametersFromFileName\n",
    "from datasetFromCsv import datasetFromCsv\n",
    "from sympy import Symbol\n",
    "from os import walk, path\n",
    "from modulus.sym.hydra import to_absolute_path\n",
    "\n",
    "\n",
    "Re = Symbol(\"Re\")\n",
    "x, y = Symbol(\"x\"), Symbol(\"y\")\n",
    "Lo, Ho = Symbol(\"Lo\"), Symbol(\"Ho\")\n",
    "\n",
    "# global ranges for my parameters\n",
    "param_ranges = {\n",
    "    Re: (100, 1000),\n",
    "    Lo: (0.1, 1),\n",
    "    Ho: (0.1, 0.5),\n",
    "    }\n",
    "\n",
    "\n",
    "ansysInvarNames = [\"Points:0\", \"Points:1\"]\n",
    "modulusInvarNames = [\"x\", \"y\"]\n",
    "ansysOutvarNames = [\"Pressure\", \"Velocity:0\", \"Velocity:1\"]\n",
    "modulusOutvarNames = [\"p_d\", \"u_d\", \"v_d\"] # these are custom node i added to separate the data loss from other losses with u,v,p they are simply defined as 1*u etc, look line 189 in fwdFacingStep\n",
    "scales = {\"p_d\": (0,1), \"u_d\": (0,1), \"v_d\": (0,1), \"x\": (0,1), \"y\": (-0.5,1)} #used to scale and translate ansys data to mach the modulus setup. in this case all variables use a scale of one and only y is translated to be centered around y=0 instead of y=0.5\n",
    "lambdaWeighting = {\"p_d\": 0.1, \"u_d\": 0.1, \"v_d\": 0.1}\n",
    "additionalConstraints=None #{\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0} \n",
    "criteria=None # if you want to use a sympy criteria to for example only use data for half the domain, same as other modulus criterias\n",
    "batches = 1000\n",
    "\n",
    "# loop through all csv files in data directory.\n",
    "for root, dirs, files in walk(to_absolute_path(\"./ansys/dataT\")):\n",
    "    for i, name in enumerate(files):\n",
    "        # read parmater values for the specific csv from its file name, requires it to be named using name_valueParameter1-valueParameter2-valueParameter3.csv\n",
    "        # any number of parameters can be used but must correspond to the ones in the global parameter range dictionary (param_ranges) in number and order.\n",
    "        dataParameterRange, shortName = readParametersFromFileName(fileName=name, parameterDict=param_ranges, generateNameString=True)\n",
    "        #dataParameterRange contains the specific parameter values, shortName is just the file name with fewer digits for each parameter.\n",
    "        \n",
    "        print(\"parameter values for file \" + name + \" :\", dataParameterRange)\n",
    "        \n",
    "        filePath = str(path.join(root, name))\n",
    "        \n",
    "        # generate a data set to be used with pointwiseConstraint.from_numpy()\n",
    "        # first a non parameterized version for reference \n",
    "        dataInvar_noParam, dataOutvar_noParam, lambdaWeights_noParam = datasetFromCsv(\n",
    "            filePath=filePath,\n",
    "            csvInvarNames=ansysInvarNames,\n",
    "            csvOutvarNames=ansysOutvarNames,\n",
    "            modulusInvarNames=modulusInvarNames,\n",
    "            modulusOutvarNames=modulusOutvarNames,\n",
    "            scales=scales,\n",
    "            parameterRanges=None,\n",
    "            criteria=criteria,\n",
    "            additionalConstraints=additionalConstraints,\n",
    "            lambdaWeighting=lambdaWeighting,\n",
    "            )\n",
    "        \n",
    "        print(\"data set for \" + name + \", non parameterized version:\\n\")\n",
    "        print(\"\\t input variables in the data set: \", dataInvar_noParam.keys())\n",
    "        print(\"\\t first 5 lines for each variable in the data set :\\n\")\n",
    "        for key in dataInvar_noParam.keys():\n",
    "            print(\"\\t\" + str(key), dataInvar_noParam[key][0:5])\n",
    "\n",
    "        \n",
    "        print(\"\\t output variables in the data set: \", dataOutvar_noParam.keys())\n",
    "        print(\"\\t first 5 lines for each variable in the data set :\\n\")\n",
    "        for key in dataOutvar_noParam.keys():\n",
    "            print(\"\\t\" + str(key), dataOutvar_noParam[key][0:5])\n",
    "        print(\"t during training the data will be structured like this input vector = output vector, although without any equal sign and order of the rows will be shuffled:\\n\")\n",
    "        for i, line in enumerate(zip(zip(dataOutvar_noParam[\"u\"], dataOutvar_noParam[\"v\"], dataOutvar_noParam[\"p\"]), zip(dataInvar_noParam[\"x\"], dataInvar_noParam[\"y\"]))):\n",
    "            lineIn=line[0]\n",
    "            lineOut=line[1]\n",
    "            print(str(lineIn) + \"=\" + str(lineOut))\n",
    "\n",
    "        dataInvar, dataOutvar, lambdaWeights = datasetFromCsv(\n",
    "            filePath=filePath,\n",
    "            csvInvarNames=ansysInvarNames,\n",
    "            csvOutvarNames=ansysOutvarNames,\n",
    "            modulusInvarNames=modulusInvarNames,\n",
    "            modulusOutvarNames=modulusOutvarNames,\n",
    "            scales=scales,\n",
    "            parameterRanges=dataParameterRange,\n",
    "            criteria=criteria,\n",
    "            additionalConstraints=additionalConstraints,\n",
    "            lambdaWeighting=lambdaWeighting,\n",
    "            )\n",
    "        \n",
    "        print(\"data set for \" + name + \", non parameterized version:\\n\")\n",
    "        print(\"\\t input variables in the data set: \", dataInvar.keys())\n",
    "        print(\"\\t first 5 lines for each variable in the data set :\\n\")\n",
    "        for key in dataInvar.keys():\n",
    "            print(\"\\t\" + str(key), dataInvar[key][0:5])\n",
    "\n",
    "        \n",
    "        print(\"\\t output variables in the data set: \", dataOutvar.keys())\n",
    "        print(\"\\t first 5 lines for each variable in the data set :\\n\")\n",
    "        for key in dataOutvar.keys():\n",
    "            print(\"\\t\" + str(key), dataOutvar[key][0:5])\n",
    "        print(\"t during training the data will be structured like this input vector = output vector, although without any equal sign and order of the rows will be shuffled:\\n\")\n",
    "        for i, line in enumerate(zip(zip(dataOutvar[\"u\"], dataOutvar[\"v\"], dataOutvar[\"p\"]), zip(dataInvar[\"x\"], dataInvar[\"y\"]))):\n",
    "            lineIn=line[0]\n",
    "            lineOut=line[1]\n",
    "            print(str(lineIn) + \"=\" + str(lineOut))\n",
    "  \n",
    "# the constraint would then be added like this, look at line 377 in fwdFacingStep\n",
    "                            \n",
    "# dataConstraint = PointwiseConstraint.from_numpy(\n",
    "#     nodes=nodes, \n",
    "#     invar=dataInvar, \n",
    "#     outvar=dataOutvar, \n",
    "#     batch_size=int(dataInvar['x'].size/batches),\n",
    "#     lambda_weighting=lambdaWeights\n",
    "# )\n",
    "\n",
    "# domain.add_constraint(dataConstraint, shortName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
